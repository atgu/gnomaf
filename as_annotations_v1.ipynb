{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f91ebd",
   "metadata": {},
   "source": [
    "# This notebook is for generating allele-specific annotations to be used in GATK VQSR Allele-Specific version for the following datasets:\n",
    "1. Nigerian genomes (54-gene): 480 genomes\n",
    "2. NeurGAP: 200 SA + 93 Other high coverage genomes\n",
    "3. Gambian Genomes Variation (GGV) Project: 394 genomes\n",
    "4. HGDP + 1KG: 4151 genomes\n",
    "5. H3Africa? GVCFs are missing RawMQandDP or (RAW_MQ, MQ_DP): reprocess as per Laura's recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad743b",
   "metadata": {},
   "source": [
    "## Annotations checks\n",
    "\n",
    "Dataset |QUALapprox |VarDP |MQ_DP and RAW_MQ or RAWMQ_andDP |SB |ReadPosRankSum |MQRankSum\n",
    "-----|-----|-----|-----|-----|-----|-----\n",
    "Nigerian genomes |9 |9 |RAW_MQandDP |1 |1 |1\n",
    "NeuroGAP SA |9 |9 |RAW_MQ |1 |1 |1\n",
    "NeuroGAP Extra |9 |9 |RAW_MQ |1 |1 |1\n",
    "GGV |9 |9 |RAW_MQandDP |1 |1 |1\n",
    "HGDP+1kGP |1 |1 |(RAW_MQ, MQ_DP)|1 |1 |1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fdfa8c",
   "metadata": {},
   "source": [
    "- From the table above, all datasets except HGDP+1kGP are missing QUALapprox and VarDP, but these can be computed from existing fields in the dataset using the compute_missing_annotations() function in this Notebook.\n",
    "- NeuroGAP genomes (SA+Extra) are also missing MQ_DP. This is a message from Laura on how to \"recover\" MQ_DP\n",
    "- MQ_DP is the depth of reads that were counted for the MQ calculation, and it is approximately equal to the INFO DP for a single sample\n",
    "    - It follows that if MQ_DP is missing, it is somewhat safe to use the INFO DP from a single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711c88b",
   "metadata": {},
   "source": [
    "## Below is the workflow\n",
    "1. Generate AS annotations for each dataset separately\n",
    "    - For the RankSum fields, we keep the frequencies (histogram) for each allele. These frequencies are used\n",
    "    downstream toapproximate the median of the merged datasets.\n",
    "2. Merge datasets\n",
    "    - AS_QUALapprox: take the sum when merging across cohorts/batches\n",
    "    - AS_VarDP: take the sum when merging across cohorts/batches\n",
    "    - AS_*_RankSum: use histograms/frequencies to approximate median\n",
    "3. Run VQSR on merged datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9111b2a",
   "metadata": {},
   "source": [
    "### RankSum histogram/frequencies\n",
    "- Below in the binning strategy used\n",
    "\n",
    "```python\n",
    "min_bound = -3\n",
    "max_bound = 3\n",
    "hist_increment = 0.01\n",
    "num_bins = int(abs(min_bound-max_bound)/hist_increment) # 600 bins in total\n",
    "```\n",
    "\n",
    "- Because the bin_edges will be the same across the datasets, it is annotated as a global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654fbb06",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2850a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Union, List, Dict, Optional, Set\n",
    "\n",
    "from gnomad.utils.annotations import (\n",
    "    fs_from_sb,\n",
    "    get_adj_expr,\n",
    "    get_lowqual_expr,\n",
    "    pab_max_expr,\n",
    "    sor_from_sb,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s (%(name)s %(lineno)s): %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %I:%M:%S %p\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344546ca",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f50ef",
   "metadata": {},
   "source": [
    "### 1.1. For checking missing annotations and computing them from existing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9545eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_annotations(fields: List[str]):\n",
    "    \"\"\"\n",
    "    Check missing annotations that are required to generate AS annotations\n",
    "    \n",
    "    :param fields: list of fields present in the dataset\n",
    "\n",
    "    :return: list of missing annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'RAW_MQandDP' in fields:\n",
    "        required_fields = ['QUALapprox', 'VarDP', 'ReadPosRankSum', 'MQRankSum', 'SB', 'RAW_MQandDP']\n",
    "    else:\n",
    "        required_fields = ['QUALapprox', 'VarDP', 'ReadPosRankSum', 'MQRankSum', 'SB', 'MQ_DP', 'RAW_MQ']\n",
    "            \n",
    "    return list(set(required_fields).difference(fields))\n",
    "    \n",
    "\n",
    "def compute_missing_annotations(vds: hl.vds.variant_dataset.VariantDataset,\n",
    "                               missing_annotations: List[str]) -> hl.vds.variant_dataset.VariantDataset:\n",
    "    \"\"\"\n",
    "    Compute VarDP and/or QUALapprox if they are missing.\n",
    "    \n",
    "    :param mt: Input VDS with missing annotations\n",
    "\n",
    "    :return: VariantDataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if ('VarDP' in missing_annotations) and ('LAD' in vds.variant_data.entry):\n",
    "        logger.info(\"Computing `VarDP` as sum(LAD)\")\n",
    "        vds.variant_data = vds.variant_data.annotate_entries(gvcf_info=\n",
    "                                                             vds.variant_data.gvcf_info.annotate(VarDP=\n",
    "                                                                                                 hl.int32(hl.sum(vds.variant_data.LAD))))\n",
    "                    \n",
    "    if ('QUALapprox' in missing_annotations) and ('LPL' in vds.variant_data.entry):\n",
    "        logger.info(\"Computing `QUALapprox` as LPL[0]\")\n",
    "        vds.variant_data = vds.variant_data.annotate_entries(gvcf_info=\n",
    "                                                             vds.variant_data.gvcf_info.annotate(QUALapprox=\n",
    "                                                                                                 vds.variant_data.LPL[0]))\n",
    "    if ('MQ_DP' in missing_annotations) and ('DP' in vds.variant_data.entry):\n",
    "        logger.info(\"Computing `MQ_DP` as DP\")\n",
    "        vds.variant_data = vds.variant_data.annotate_entries(gvcf_info=\n",
    "                                                             vds.variant_data.gvcf_info.annotate(MQ_DP=\n",
    "                                                                                                 vds.variant_data.DP))\n",
    "                    \n",
    "    return vds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296953cb",
   "metadata": {},
   "source": [
    "### 1.2 gnomAD functions (made a slight change to make sure we are computing histogram for RankSum annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8ca5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_AGG_FIELDS = {\n",
    "    \"sum_agg_fields\": [\"QUALapprox\"], # take the sum when merging across cohorts/batches\n",
    "    \"int32_sum_agg_fields\": [\"VarDP\"], # take the sum when merging across cohorts/batches\n",
    "    \"median_agg_fields\": [\"ReadPosRankSum\", \"MQRankSum\"], # get frequencies by bin for each cohort and compute median after merging\n",
    "    \"array_sum_agg_fields\": [\"SB\", \"RAW_MQandDP\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeff475",
   "metadata": {},
   "source": [
    "**Modify \\_get_info_agg_expr() such that it computes the histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ace678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_info_agg_expr(\n",
    "    mt: hl.MatrixTable,\n",
    "    sum_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.NumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"sum_agg_fields\"],\n",
    "    int32_sum_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.NumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"int32_sum_agg_fields\"],\n",
    "    median_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.NumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"median_agg_fields\"],\n",
    "    array_sum_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.ArrayNumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"array_sum_agg_fields\"],\n",
    "    prefix: str = \"\",\n",
    "    treat_fields_as_allele_specific: bool = False,\n",
    ") -> Dict[str, hl.expr.Aggregation]:\n",
    "    \"\"\"\n",
    "    Create Aggregators for both site or AS info expression aggregations.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        - If `SB` is specified in array_sum_agg_fields, it will be aggregated as\n",
    "          `AS_SB_TABLE`, according to GATK standard nomenclature.\n",
    "        - If `RAW_MQandDP` is specified in array_sum_agg_fields, it will be used for\n",
    "          the `MQ` calculation and then dropped according to GATK recommendation.\n",
    "        - If `RAW_MQ` and `MQ_DP` are given, they will be used for the `MQ` calculation\n",
    "          and then dropped according to GATK recommendation.\n",
    "        - If the fields to be aggregated (`sum_agg_fields`, `int32_sum_agg_fields`,\n",
    "          `median_agg_fields`) are passed as list of str, then they should correspond\n",
    "          to entry fields in `mt` or in mt.gvcf_info`.\n",
    "        - Priority is given to entry fields in `mt` over those in `mt.gvcf_info` in\n",
    "          case of a name clash.\n",
    "\n",
    "    :param mt: Input MT\n",
    "    :param sum_agg_fields: Fields to aggregate using sum.\n",
    "    :param int32_sum_agg_fields: Fields to aggregate using sum using int32.\n",
    "    :param median_agg_fields: Fields to aggregate using (approximate) median.\n",
    "    :param array_sum_agg_fields: Fields to aggregate using element-wise summing over an\n",
    "        array.\n",
    "    :param prefix: Optional prefix for the fields. Used for adding 'AS_' in the AS case.\n",
    "    :param treat_fields_as_allele_specific: Treat info fields as allele-specific. Defaults to False.\n",
    "    :return: Dictionary of expression names and their corresponding aggregation\n",
    "        Expression.\n",
    "    \"\"\"\n",
    "\n",
    "    def _agg_list_to_dict(\n",
    "        mt: hl.MatrixTable, fields: List[str]\n",
    "    ) -> Dict[str, hl.expr.NumericExpression]:\n",
    "        out_fields = {}\n",
    "        if \"gvcf_info\" in mt.entry:\n",
    "            out_fields = {f: mt.gvcf_info[f] for f in fields if f in mt.gvcf_info}\n",
    "\n",
    "        out_fields.update({f: mt[f] for f in fields if f in mt.entry})\n",
    "\n",
    "        # Check that all fields were found.\n",
    "        missing_fields = [f for f in fields if f not in out_fields]\n",
    "        if missing_fields:\n",
    "            raise ValueError(\n",
    "                \"Could not find the following field(s)in the MT entry schema (or nested\"\n",
    "                \" under mt.gvcf_info: {}\".format(\",\".join(missing_fields))\n",
    "            )\n",
    "\n",
    "        if treat_fields_as_allele_specific:\n",
    "            # TODO: Change to use hl.vds.local_to_global when fill_value can accept\n",
    "            #  missing (error in v0.2.119).\n",
    "            out_fields = {\n",
    "                f: hl.bind(\n",
    "                    lambda x: hl.if_else(f == \"AS_SB_TABLE\", x, x[1:]),\n",
    "                    hl.range(hl.len(mt.alleles)).map(\n",
    "                        lambda i: hl.or_missing(\n",
    "                            mt.LA.contains(i), out_fields[f][mt.LA.index(i)]\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "                for f in fields\n",
    "            }\n",
    "\n",
    "        return out_fields\n",
    "\n",
    "    # Map str to expressions where needed.\n",
    "    if isinstance(sum_agg_fields, list):\n",
    "        sum_agg_fields = _agg_list_to_dict(mt, sum_agg_fields)\n",
    "\n",
    "    if isinstance(int32_sum_agg_fields, list):\n",
    "        int32_sum_agg_fields = _agg_list_to_dict(mt, int32_sum_agg_fields)\n",
    "\n",
    "    if isinstance(median_agg_fields, list):\n",
    "        median_agg_fields = _agg_list_to_dict(mt, median_agg_fields)\n",
    "\n",
    "    if isinstance(array_sum_agg_fields, list):\n",
    "        array_sum_agg_fields = _agg_list_to_dict(mt, array_sum_agg_fields)\n",
    "    \n",
    "    # For median_agg_fields, compute histogram instead of approximating quantiles\n",
    "    aggs = [\n",
    "        # (median_agg_fields, lambda x: hl.agg.approx_quantiles(x, 0.5)),\n",
    "        (median_agg_fields, lambda x: hl.agg.hist(x, -3, 3, 600)),\n",
    "        (sum_agg_fields, hl.agg.sum),\n",
    "        (int32_sum_agg_fields, lambda x: hl.int32(hl.agg.sum(x))),\n",
    "        (array_sum_agg_fields, hl.agg.array_sum),\n",
    "    ]\n",
    "\n",
    "    # Create aggregators.\n",
    "    agg_expr = {}\n",
    "    for agg_fields, agg_func in aggs:\n",
    "        for k, expr in agg_fields.items():\n",
    "            if treat_fields_as_allele_specific:\n",
    "                # If annotation is of the form 'AS_RAW_*_RankSum' it has a histogram\n",
    "                # representation where keys give the per-variant rank sum value to one\n",
    "                # decimal place followed by a comma and the corresponding count for\n",
    "                # that value, so we want to sum the rank sum value (first element).\n",
    "                # Rename annotation in the form 'AS_RAW_*_RankSum' to 'AS_*_RankSum'.\n",
    "                if k.startswith(\"AS_RAW_\") and k.endswith(\"RankSum\"):\n",
    "                    agg_expr[f\"{prefix}{k.replace('_RAW', '')}\"] = hl.agg.array_agg(\n",
    "                        lambda x: agg_func(hl.or_missing(hl.is_defined(x), x[0])), expr\n",
    "                    )\n",
    "                else:\n",
    "                    agg_expr[f\"{prefix}{k}\"] = hl.agg.array_agg(\n",
    "                        lambda x: agg_func(x), expr\n",
    "                    )\n",
    "            else:\n",
    "                agg_expr[f\"{prefix}{k}\"] = agg_func(expr)\n",
    "\n",
    "    if treat_fields_as_allele_specific:\n",
    "        prefix = \"AS_\"\n",
    "\n",
    "    # Handle annotations combinations and casting for specific annotations\n",
    "    # If RAW_MQandDP is in agg_expr or if both MQ_DP and RAW_MQ are, compute MQ instead\n",
    "    mq_tuple = None\n",
    "    if f\"{prefix}RAW_MQandDP\" in agg_expr:\n",
    "        logger.info(\n",
    "            \"Computing %sMQ as sqrt(%sRAW_MQandDP[0]/%sRAW_MQandDP[1]). \"\n",
    "            \"Note that %sMQ will be set to 0 if %sRAW_MQandDP[1] == 0.\",\n",
    "            *[prefix] * 5,\n",
    "        )\n",
    "        mq_tuple = agg_expr.pop(f\"{prefix}RAW_MQandDP\")\n",
    "    elif \"AS_RAW_MQ\" in agg_expr and treat_fields_as_allele_specific:\n",
    "        logger.info(\n",
    "            \"Computing AS_MQ as sqrt(AS_RAW_MQ[i]/AD[i+1]). \"\n",
    "            \"Note that AS_MQ will be set to 0 if AS_RAW_MQ == 0.\"\n",
    "        )\n",
    "        ad_expr = hl.vds.local_to_global(\n",
    "            mt.LAD, mt.LA, hl.len(mt.alleles), fill_value=0, number=\"R\"\n",
    "        )\n",
    "        mq_tuple = hl.zip(agg_expr.pop(\"AS_RAW_MQ\"), hl.agg.array_sum(ad_expr[1:]))\n",
    "    elif f\"{prefix}RAW_MQ\" in agg_expr and f\"{prefix}MQ_DP\" in agg_expr:\n",
    "        logger.info(\n",
    "            \"Computing %sMQ as sqrt(%sRAW_MQ/%sMQ_DP). \"\n",
    "            \"Note that MQ will be set to 0 if %sRAW_MQ == 0.\",\n",
    "            *[prefix] * 4,\n",
    "        )\n",
    "        mq_tuple = (agg_expr.pop(f\"{prefix}RAW_MQ\"), agg_expr.pop(f\"{prefix}MQ_DP\"))\n",
    "\n",
    "    if mq_tuple is not None:\n",
    "        if treat_fields_as_allele_specific:\n",
    "            agg_expr[f\"{prefix}MQ\"] = mq_tuple.map(\n",
    "                lambda x: hl.if_else(x[1] > 0, hl.sqrt(x[0] / x[1]), 0)\n",
    "            )\n",
    "        else:\n",
    "            agg_expr[f\"{prefix}MQ\"] = hl.if_else(\n",
    "                mq_tuple[1] > 0, hl.sqrt(mq_tuple[0] / mq_tuple[1]), 0\n",
    "            )\n",
    "\n",
    "    # If both VarDP and QUALapprox are present, also compute QD.\n",
    "    if f\"{prefix}VarDP\" in agg_expr and f\"{prefix}QUALapprox\" in agg_expr:\n",
    "        logger.info(\n",
    "            \"Computing %sQD as %sQUALapprox/%sVarDP. \"\n",
    "            \"Note that %sQD will be set to 0 if %sVarDP == 0.\",\n",
    "            *[prefix] * 5,\n",
    "        )\n",
    "        var_dp = agg_expr[f\"{prefix}VarDP\"]\n",
    "        qual_approx = agg_expr[f\"{prefix}QUALapprox\"]\n",
    "        if treat_fields_as_allele_specific:\n",
    "            agg_expr[f\"{prefix}QD\"] = hl.map(\n",
    "                lambda x: hl.if_else(x[1] > 0, x[0] / x[1], 0),\n",
    "                hl.zip(qual_approx, var_dp),\n",
    "            )\n",
    "        else:\n",
    "            agg_expr[f\"{prefix}QD\"] = hl.if_else(var_dp > 0, qual_approx / var_dp, 0)\n",
    "\n",
    "    # SB needs to be cast to int32 for FS down the line.\n",
    "    if f\"{prefix}SB\" in agg_expr:\n",
    "        agg_expr[f\"{prefix}SB\"] = agg_expr[f\"{prefix}SB\"].map(lambda x: hl.int32(x))\n",
    "\n",
    "    # SB needs to be cast to int32 for FS down the line.\n",
    "    if \"AS_SB_TABLE\" in agg_expr:\n",
    "        agg_expr[\"AS_SB_TABLE\"] = agg_expr[\"AS_SB_TABLE\"].map(\n",
    "            lambda x: x.map(lambda y: hl.int32(y))\n",
    "        )\n",
    "\n",
    "    return agg_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854d5d6",
   "metadata": {},
   "source": [
    "**We do not import get_as_info_expr() directly from gnomAD methods because it calls \\_get_info_agg_expr we modified above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae666fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_as_info_expr(\n",
    "    mt: hl.MatrixTable,\n",
    "    sum_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.NumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"sum_agg_fields\"],\n",
    "    int32_sum_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.NumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"int32_sum_agg_fields\"],\n",
    "    median_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.NumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"median_agg_fields\"],\n",
    "    array_sum_agg_fields: Union[\n",
    "        List[str], Dict[str, hl.expr.ArrayNumericExpression]\n",
    "    ] = INFO_AGG_FIELDS[\"array_sum_agg_fields\"],\n",
    "    alt_alleles_range_array_field: str = \"alt_alleles_range_array\",\n",
    "    treat_fields_as_allele_specific: bool = False,\n",
    ") -> hl.expr.StructExpression:\n",
    "    \"\"\"\n",
    "    Return an allele-specific annotation Struct containing typical VCF INFO fields from GVCF INFO fields stored in the MT entries.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        - If `SB` is specified in array_sum_agg_fields, it will be aggregated as\n",
    "          `AS_SB_TABLE`, according to GATK standard nomenclature.\n",
    "        - If `RAW_MQandDP` is specified in array_sum_agg_fields, it will be used for\n",
    "          the `MQ` calculation and then dropped according to GATK recommendation.\n",
    "        - If `RAW_MQ` and `MQ_DP` are given, they will be used for the `MQ` calculation\n",
    "          and then dropped according to GATK recommendation.\n",
    "        - If the fields to be aggregate (`sum_agg_fields`, `int32_sum_agg_fields`,\n",
    "          `median_agg_fields`) are passed as list of str, then they should correspond\n",
    "          to entry fields in `mt` or in `mt.gvcf_info`.\n",
    "        - Priority is given to entry fields in `mt` over those in `mt.gvcf_info` in\n",
    "          case of a name clash.\n",
    "        - If `treat_fields_as_allele_specific` is False, it's expected that there is a\n",
    "          single value for each entry field to be aggregated. Then when performing the\n",
    "          aggregation per global alternate allele, that value is included in the\n",
    "          aggregation if the global allele is present in the entry's list of local\n",
    "          alleles. If `treat_fields_as_allele_specific` is True, it's expected that\n",
    "          each entry field to be aggregated has one value per local allele, and each\n",
    "          of those is mapped to a global allele for aggregation.\n",
    "\n",
    "    :param mt: Input Matrix Table\n",
    "    :param sum_agg_fields: Fields to aggregate using sum.\n",
    "    :param int32_sum_agg_fields: Fields to aggregate using sum using int32.\n",
    "    :param median_agg_fields: Fields to aggregate using (approximate) median.\n",
    "    :param array_sum_agg_fields: Fields to aggregate using array sum.\n",
    "    :param alt_alleles_range_array_field: Annotation containing an array of the range\n",
    "        of alternate alleles e.g., `hl.range(1, hl.len(mt.alleles))`\n",
    "    :param treat_fields_as_allele_specific: Treat info fields as allele-specific.\n",
    "        Defaults to False.\n",
    "    :return: Expression containing the AS info fields\n",
    "    \"\"\"\n",
    "    if \"DP\" in list(sum_agg_fields) + list(int32_sum_agg_fields):\n",
    "        logger.warning(\n",
    "            \"`DP` was included in allele-specific aggregation, however `DP` is\"\n",
    "            \" typically not aggregated by allele; `VarDP` is.Note that the resulting\"\n",
    "            \" `AS_DP` field will NOT include reference genotypes.\"\n",
    "        )\n",
    "\n",
    "    agg_expr = _get_info_agg_expr(\n",
    "        mt=mt,\n",
    "        sum_agg_fields=sum_agg_fields,\n",
    "        int32_sum_agg_fields=int32_sum_agg_fields,\n",
    "        median_agg_fields=median_agg_fields,\n",
    "        array_sum_agg_fields=array_sum_agg_fields,\n",
    "        prefix=\"\" if treat_fields_as_allele_specific else \"AS_\",\n",
    "        treat_fields_as_allele_specific=treat_fields_as_allele_specific,\n",
    "    )\n",
    "\n",
    "    if alt_alleles_range_array_field not in mt.row or mt[\n",
    "        alt_alleles_range_array_field\n",
    "    ].dtype != hl.dtype(\"array<int32>\"):\n",
    "        msg = (\n",
    "            f\"'get_as_info_expr' expected a row field '{alt_alleles_range_array_field}'\"\n",
    "            \" of type array<int32>\"\n",
    "        )\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if not treat_fields_as_allele_specific:\n",
    "        # Modify aggregations to aggregate per allele\n",
    "        agg_expr = {\n",
    "            f: hl.agg.array_agg(\n",
    "                lambda ai: hl.agg.filter(mt.LA.contains(ai), expr),\n",
    "                mt[alt_alleles_range_array_field],\n",
    "            )\n",
    "            for f, expr in agg_expr.items()\n",
    "        }\n",
    "\n",
    "    # Run aggregations\n",
    "    info = hl.struct(**agg_expr)\n",
    "\n",
    "    # Add FS and SOR if SB is present.\n",
    "    if \"AS_SB_TABLE\" in info or \"AS_SB\" in info:\n",
    "        # Rename AS_SB to AS_SB_TABLE if present and add SB Ax2 aggregation logic.\n",
    "        if \"AS_SB\" in agg_expr:\n",
    "            if \"AS_SB_TABLE\" in agg_expr:\n",
    "                logger.warning(\n",
    "                    \"Both `AS_SB` and `AS_SB_TABLE` were specified for aggregation.\"\n",
    "                    \" `AS_SB` will be used for aggregation.\"\n",
    "                )\n",
    "            as_sb_table = hl.array(\n",
    "                [\n",
    "                    info.AS_SB.filter(lambda x: hl.is_defined(x)).fold(\n",
    "                        lambda i, j: i[:2] + j[:2], [0, 0]\n",
    "                    )  # ref\n",
    "                ]\n",
    "            ).extend(\n",
    "                info.AS_SB.map(lambda x: x[2:])  # each alt\n",
    "            )\n",
    "        else:\n",
    "            as_sb_table = info.AS_SB_TABLE\n",
    "        info = info.annotate(\n",
    "            AS_SB_TABLE=as_sb_table,\n",
    "            AS_FS=hl.range(1, hl.len(mt.alleles)).map(\n",
    "                lambda i: fs_from_sb(as_sb_table[0].extend(as_sb_table[i]))\n",
    "            ),\n",
    "            AS_SOR=hl.range(1, hl.len(mt.alleles)).map(\n",
    "                lambda i: sor_from_sb(as_sb_table[0].extend(as_sb_table[i]))\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dbf9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_compute_info(\n",
    "    mt: hl.MatrixTable,\n",
    "    site_annotations: bool = False,\n",
    "    as_annotations: bool = False,\n",
    "    # Set to True by default to prevent a breaking change.\n",
    "    quasi_as_annotations: bool = True,\n",
    "    n_partitions: int = 5000,\n",
    "    lowqual_indel_phred_het_prior: int = 40,\n",
    "    ac_filter_groups: Optional[Dict[str, hl.Expression]] = None,\n",
    ") -> hl.Table:\n",
    "    \"\"\"\n",
    "    Compute a HT with the typical GATK allele-specific (AS) info fields as well as ACs and lowqual fields.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        - This table doesn't split multi-allelic sites.\n",
    "        - At least one of `site_annotations`, `as_annotations` or `quasi_as_annotations`\n",
    "          must be True.\n",
    "\n",
    "    :param mt: Input MatrixTable. Note that this table should be filtered to nonref sites.\n",
    "    :param site_annotations: Whether to generate site level info fields. Default is False.\n",
    "    :param as_annotations: Whether to generate allele-specific info fields using\n",
    "        allele-specific annotations in gvcf_info. Default is False.\n",
    "    :param quasi_as_annotations: Whether to generate allele-specific info fields using\n",
    "        non-allele-specific annotations in gvcf_info, but performing per allele\n",
    "        aggregations. This method can be used in cases where genotype data doesn't\n",
    "        contain allele-specific annotations to approximate allele-specific annotations.\n",
    "        Default is True.\n",
    "    :param n_partitions: Number of desired partitions for output Table. Default is 5000.\n",
    "    :param lowqual_indel_phred_het_prior: Phred-scaled prior for a het genotype at a\n",
    "        site with a low quality indel. Default is 40. We use 1/10k bases (phred=40) to\n",
    "        be more consistent with the filtering used by Broad's Data Sciences Platform\n",
    "        for VQSR.\n",
    "    :param ac_filter_groups: Optional dictionary of sample filter expressions to compute\n",
    "        additional groupings of ACs. Default is None.\n",
    "    :return: Table with info fields\n",
    "    :rtype: Table\n",
    "    \"\"\"\n",
    "    if not site_annotations and not as_annotations and not quasi_as_annotations:\n",
    "        raise ValueError(\n",
    "            \"At least one of `site_annotations`, `as_annotations`, or \"\n",
    "            \"`quasi_as_annotations` must be True!\"\n",
    "        )\n",
    "\n",
    "    # Add a temporary annotation for allele count groupings.\n",
    "    ac_filter_groups = {\"\": True, **(ac_filter_groups or {})}\n",
    "    mt = mt.annotate_cols(_ac_filter_groups=ac_filter_groups)\n",
    "\n",
    "    # Move gvcf info entries out from nested struct.\n",
    "    mt = mt.transmute_entries(**mt.gvcf_info)\n",
    "\n",
    "    # Adding alt_alleles_range_array as a required annotation for\n",
    "    # get_as_info_expr to reduce memory usage.\n",
    "    mt = mt.annotate_rows(alt_alleles_range_array=hl.range(1, hl.len(mt.alleles)))\n",
    "\n",
    "    info_expr = None\n",
    "    quasi_info_expr = None\n",
    "\n",
    "    # Compute quasi-AS info expr.\n",
    "    if quasi_as_annotations:\n",
    "        info_expr = get_as_info_expr(mt)\n",
    "\n",
    "    # Compute AS info expr using gvcf_info allele specific annotations.\n",
    "    if as_annotations:\n",
    "        if info_expr is not None:\n",
    "            quasi_info_expr = info_expr\n",
    "        info_expr = get_as_info_expr(\n",
    "            mt,\n",
    "            **AS_INFO_AGG_FIELDS,\n",
    "            treat_fields_as_allele_specific=True,\n",
    "        )\n",
    "\n",
    "    if info_expr is not None:\n",
    "        # Add allele specific pab_max\n",
    "        info_expr = info_expr.annotate(\n",
    "            AS_pab_max=pab_max_expr(mt.LGT, mt.LAD, mt.LA, hl.len(mt.alleles))\n",
    "        )\n",
    "\n",
    "    if site_annotations:\n",
    "        site_expr = get_site_info_expr(mt)\n",
    "        if info_expr is None:\n",
    "            info_expr = site_expr\n",
    "        else:\n",
    "            info_expr = info_expr.annotate(**site_expr)\n",
    "\n",
    "    # Add 'AC' and 'AC_raw' for each allele count filter group requested.\n",
    "    # First compute ACs for each non-ref allele, grouped by adj.\n",
    "    grp_ac_expr = {\n",
    "        f: hl.agg.array_agg(\n",
    "            lambda ai: hl.agg.filter(\n",
    "                mt.LA.contains(ai) & mt._ac_filter_groups[f],\n",
    "                hl.agg.group_by(\n",
    "                    get_adj_expr(mt.LGT, mt.GQ, mt.DP, mt.LAD),\n",
    "                    hl.agg.sum(\n",
    "                        mt.LGT.one_hot_alleles(mt.LA.map(lambda x: hl.str(x)))[\n",
    "                            mt.LA.index(ai)\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            mt.alt_alleles_range_array,\n",
    "        )\n",
    "        for f in ac_filter_groups\n",
    "    }\n",
    "\n",
    "    # Then, for each non-ref allele, compute\n",
    "    # 'AC' as the adj group\n",
    "    # 'AC_raw' as the sum of adj and non-adj groups\n",
    "    info_expr = info_expr.annotate(\n",
    "        **{\n",
    "            f\"AC{'_' + f if f else f}_raw\": grp.map(\n",
    "                lambda i: hl.int32(i.get(True, 0) + i.get(False, 0))\n",
    "            )\n",
    "            for f, grp in grp_ac_expr.items()\n",
    "        },\n",
    "        **{\n",
    "            f\"AC{'_' + f if f else f}\": grp.map(lambda i: hl.int32(i.get(True, 0)))\n",
    "            for f, grp in grp_ac_expr.items()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ann_expr = {\"info\": info_expr}\n",
    "    if quasi_info_expr is not None:\n",
    "        ann_expr[\"quasi_info\"] = quasi_info_expr\n",
    "\n",
    "    info_ht = mt.select_rows(**ann_expr).rows()\n",
    "\n",
    "    # Add AS lowqual flag\n",
    "    info_ht = info_ht.annotate(\n",
    "        AS_lowqual=get_lowqual_expr(\n",
    "            info_ht.alleles,\n",
    "            info_ht.info.AS_QUALapprox,\n",
    "            indel_phred_het_prior=lowqual_indel_phred_het_prior,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if site_annotations:\n",
    "        # Add lowqual flag\n",
    "        info_ht = info_ht.annotate(\n",
    "            lowqual=get_lowqual_expr(\n",
    "                info_ht.alleles,\n",
    "                info_ht.info.QUALapprox,\n",
    "                indel_phred_het_prior=lowqual_indel_phred_het_prior,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return info_ht.naive_coalesce(n_partitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd7ec5",
   "metadata": {},
   "source": [
    "### Function for selecting only frequencies after computing AS annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ebdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_table_cleanup(as_table: hl.Table,\n",
    "                    start: int = -3,\n",
    "                    end: int = 3,\n",
    "                    step: float = 0.01\n",
    "                    ) -> hl.Table:\n",
    "    \"\"\"\n",
    "    Only keep frequencies and remove other fields (bin_edges, n_smaller/bigger) computed by hl.agg.hist() to save space\n",
    "    \n",
    "    :param as_table: Input HT with AS annotations, including AS_*_RankSum\n",
    "    :param start: Start of histogram range.\n",
    "    :param end: End of histogram range.\n",
    "    :param step: Difference between any two subsequent numbers\n",
    "\n",
    "    :return: Hail Table with AS_*_RankSum fields containing only frequencies\n",
    "    \"\"\"\n",
    "    \n",
    "    as_table = as_table.annotate_globals(bin_edges = [hl.float64(i) for i in np.arange(start, end, step)])\n",
    "    \n",
    "    # only keep bin frequencies and drop the other elements of the struct i.e. overwrite AS_*_RankSum with just frequency\n",
    "    as_table = as_table.annotate(info = as_table.info.annotate(AS_ReadPosRankSum_freq = as_table.info.AS_ReadPosRankSum['bin_freq']))\n",
    "    as_table = as_table.annotate(info = as_table.info.annotate(AS_MQRankSum_freq = as_table.info.AS_MQRankSum['bin_freq']))\n",
    "    \n",
    "    # drop the AS_*_RankSum that has all information to save space\n",
    "    as_table = as_table.annotate(info = as_table.info.drop('AS_ReadPosRankSum', 'AS_MQRankSum'))\n",
    "    \n",
    "    return as_table\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f74a74",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dea86b",
   "metadata": {},
   "source": [
    "### 1. Nigerian genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c54ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://gnomaf-annotations-m.c.diverse-pop-seq-ref.internal:40749\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.115-10932c754edb\n",
      "LOGGING: writing to /home/hail/hail-20230829-2016-0.2.115-10932c754edb.log\n",
      "2023-08-29 20:16:41.030 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "vds_nig = hl.vds.read_vds('gs://nigeria-54gene/combined-gvcfs/nigeria_54gene_merged_gvcfs.vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6551fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94990636, 480)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_nig.variant_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097dca0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VarDP', 'QUALapprox']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_nigeria = check_missing_annotations(list(vds_nig.variant_data.gvcf_info) + list(vds_nig.variant_data.entry))\n",
    "missing_nigeria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "453dca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:17:09 PM (__main__ 29): Computing `VarDP` as sum(LAD)\n",
      "08/29/2023 08:17:09 PM (__main__ 35): Computing `QUALapprox` as LPL[0]\n"
     ]
    }
   ],
   "source": [
    "# computing missing annotations\n",
    "vds_nig = compute_missing_annotations(vds_nig, missing_nigeria)\n",
    "\n",
    "# data to be passed to default_compute_info should be filtered to nonref sites\n",
    "mt_nig = vds_nig.variant_data.filter_entries(vds_nig.variant_data.LGT.is_non_ref(), keep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951ef62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:17:31 PM (__main__ 131): Computing AS_MQ as sqrt(AS_RAW_MQandDP[0]/AS_RAW_MQandDP[1]). Note that AS_MQ will be set to 0 if AS_RAW_MQandDP[1] == 0.\n",
      "08/29/2023 08:17:31 PM (__main__ 166): Computing AS_QD as AS_QUALapprox/AS_VarDP. Note that AS_QD will be set to 0 if AS_VarDP == 0.\n"
     ]
    }
   ],
   "source": [
    "# compute AS annotation\n",
    "as_ht_nigeria = default_compute_info(mt_nig)\n",
    "\n",
    "# only keep frequencies for the RankSum annotations\n",
    "as_ht_nigeria = as_table_cleanup(as_ht_nigeria)\n",
    "\n",
    "# as_ht_nigeria.filter(as_ht_nigeria.info.AS_ReadPosRankSum_freq[0][0] > 0).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c30f74",
   "metadata": {},
   "source": [
    "### 2.1 NeuroGAP SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e99948aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 20:18:09.854 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "vds_sa = hl.vds.read_vds('gs://neurogap-highcov-genomes/SA-genomes/south_african_genomes_merged_gvcfs.vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653c2e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126525458, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_sa.variant_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6391da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MQ_DP', 'VarDP', 'QUALapprox']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_sa = check_missing_annotations(list(vds_sa.variant_data.gvcf_info) + list(vds_sa.variant_data.entry))\n",
    "missing_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21306928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:19:02 PM (__main__ 29): Computing `VarDP` as sum(LAD)\n",
      "08/29/2023 08:19:02 PM (__main__ 35): Computing `QUALapprox` as LPL[0]\n",
      "08/29/2023 08:19:02 PM (__main__ 40): Computing `MQ_DP` as DP\n"
     ]
    }
   ],
   "source": [
    "# computing missing annotations\n",
    "vds_sa = compute_missing_annotations(vds_sa, missing_sa)\n",
    "\n",
    "# data to be passed to default_compute_info should be filtered to nonref sites\n",
    "mt_sa = vds_sa.variant_data.filter_entries(vds_sa.variant_data.LGT.is_non_ref(), keep=True)\n",
    "\n",
    "# We have RAW_MQ and MQ_DP as separate annotations but default_compute_info looks for RAW_MQandDP \n",
    "mt_sa = mt_sa.annotate_entries(gvcf_info = mt_sa.gvcf_info.annotate(RAW_MQandDP =\n",
    "                                                        hl.array([mt_sa.gvcf_info.RAW_MQ, mt_sa.gvcf_info.MQ_DP])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fe01046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:19:35 PM (__main__ 131): Computing AS_MQ as sqrt(AS_RAW_MQandDP[0]/AS_RAW_MQandDP[1]). Note that AS_MQ will be set to 0 if AS_RAW_MQandDP[1] == 0.\n",
      "08/29/2023 08:19:35 PM (__main__ 166): Computing AS_QD as AS_QUALapprox/AS_VarDP. Note that AS_QD will be set to 0 if AS_VarDP == 0.\n"
     ]
    }
   ],
   "source": [
    "# compute AS annotation\n",
    "as_ht_sa = default_compute_info(mt_sa)\n",
    "\n",
    "# only keep frequencies for the RankSum annotations\n",
    "as_ht_sa = as_table_cleanup(as_ht_sa)\n",
    "\n",
    "# as_ht_sa.filter(as_ht_sa.info.AS_ReadPosRankSum_freq[0][0] > 0).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dba1e1",
   "metadata": {},
   "source": [
    "### 2.2 NeuroGAP Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2c84bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 20:19:47.061 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "vds_neuro_extra = hl.vds.read_vds('gs://neurogap-highcov-genomes/NeuroGAP-extra-93-genomes/neurogap_highcov_all_sites_93_genomes_merged_gvcfs.vds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "471bc8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88618041, 93)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_neuro_extra.variant_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25d51d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MQ_DP', 'VarDP', 'QUALapprox']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_neuro_extra = check_missing_annotations(list(vds_neuro_extra.variant_data.gvcf_info) + list(vds_neuro_extra.variant_data.entry))\n",
    "missing_neuro_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c0b8fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:21:32 PM (__main__ 29): Computing `VarDP` as sum(LAD)\n",
      "08/29/2023 08:21:32 PM (__main__ 35): Computing `QUALapprox` as LPL[0]\n",
      "08/29/2023 08:21:32 PM (__main__ 40): Computing `MQ_DP` as DP\n"
     ]
    }
   ],
   "source": [
    "# computing missing annotations\n",
    "vds_neuro_extra = compute_missing_annotations(vds_neuro_extra, missing_neuro_extra)\n",
    "\n",
    "# data to be passed to default_compute_info should be filtered to nonref sites\n",
    "mt_neuro_extra = vds_neuro_extra.variant_data.filter_entries(vds_neuro_extra.variant_data.LGT.is_non_ref(),\n",
    "                                                             keep=True)\n",
    "\n",
    "# We have RAW_MQ and MQ_DP as separate annotations but default_compute_info looks for RAW_MQandDP \n",
    "mt_neuro_extra = mt_neuro_extra.annotate_entries(gvcf_info =\n",
    "                                                 mt_neuro_extra.gvcf_info.annotate(RAW_MQandDP =\n",
    "                                                        hl.array([mt_neuro_extra.gvcf_info.RAW_MQ,\n",
    "                                                                  mt_neuro_extra.gvcf_info.MQ_DP])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d93a4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:21:52 PM (__main__ 131): Computing AS_MQ as sqrt(AS_RAW_MQandDP[0]/AS_RAW_MQandDP[1]). Note that AS_MQ will be set to 0 if AS_RAW_MQandDP[1] == 0.\n",
      "08/29/2023 08:21:52 PM (__main__ 166): Computing AS_QD as AS_QUALapprox/AS_VarDP. Note that AS_QD will be set to 0 if AS_VarDP == 0.\n"
     ]
    }
   ],
   "source": [
    "# compute AS annotation\n",
    "as_ht_neuro_extra = default_compute_info(mt_neuro_extra)\n",
    "\n",
    "# only keep frequencies for the RankSum annotations\n",
    "as_ht_neuro_extra = as_table_cleanup(as_ht_neuro_extra)\n",
    "\n",
    "# as_ht_neuro_extra.filter(as_ht_neuro_extra.info.AS_ReadPosRankSum_freq[0][0] > 0).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f887",
   "metadata": {},
   "source": [
    "### 3. GGV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb8bc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 20:22:03.645 Hail: WARN: You are reading a VDS written with an older version of Hail.\n",
      "  Hail now supports much faster interval filters on VDS, but you'll need to run either\n",
      "  `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the\n",
      "  existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.\n"
     ]
    }
   ],
   "source": [
    "vds_ggv = hl.vds.read_vds('gs://gnomaf/gambian-genomes/COMBINED_GVCFS/gambian_genomes_merged_gvcfs.vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5295a8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61164017, 394)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds_ggv.variant_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f188011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VarDP', 'QUALapprox']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_ggv = check_missing_annotations(list(vds_ggv.variant_data.gvcf_info) + list(vds_ggv.variant_data.entry))\n",
    "missing_ggv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68cc9bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:23:25 PM (__main__ 29): Computing `VarDP` as sum(LAD)\n",
      "08/29/2023 08:23:25 PM (__main__ 35): Computing `QUALapprox` as LPL[0]\n"
     ]
    }
   ],
   "source": [
    "# computing missing annotations\n",
    "vds_ggv = compute_missing_annotations(vds_ggv, missing_ggv)\n",
    "\n",
    "# data to be passed to default_compute_info should be filtered to nonref sites\n",
    "mt_ggv = vds_ggv.variant_data.filter_entries(vds_ggv.variant_data.LGT.is_non_ref(), keep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e35cbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:23:40 PM (__main__ 131): Computing AS_MQ as sqrt(AS_RAW_MQandDP[0]/AS_RAW_MQandDP[1]). Note that AS_MQ will be set to 0 if AS_RAW_MQandDP[1] == 0.\n",
      "08/29/2023 08:23:40 PM (__main__ 166): Computing AS_QD as AS_QUALapprox/AS_VarDP. Note that AS_QD will be set to 0 if AS_VarDP == 0.\n"
     ]
    }
   ],
   "source": [
    "# compute AS annotation\n",
    "as_ht_ggv = default_compute_info(mt_ggv)\n",
    "\n",
    "# only keep frequencies for the RankSum annotations\n",
    "as_ht_ggv = as_table_cleanup(as_ht_ggv)\n",
    "\n",
    "# as_ht_ggv.filter(as_ht_ggv.info.AS_ReadPosRankSum_freq[0][0] > 0).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05644c50",
   "metadata": {},
   "source": [
    "### 4. HGDP+1kGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70f13ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_hgdp_tgp = hl.read_matrix_table('gs://gcp-public-data--gnomad/release/3.1.2/mt/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_sparse.mt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1844c74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1538715096, 4151)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_hgdp_tgp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdb10e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_hgdp_tgp = check_missing_annotations(list(mt_hgdp_tgp.entry.gvcf_info) + list(mt_hgdp_tgp.entry))\n",
    "missing_hgdp_tgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff52199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to be passed to default_compute_info should be filtered to nonref sites\n",
    "mt_hgdp_tgp = mt_hgdp_tgp.filter_entries(mt_hgdp_tgp.LGT.is_non_ref(), keep=True)\n",
    "\n",
    "# We have RAW_MQ and MQ_DP as separate annotations but default_compute_info looks for RAW_MQandDP \n",
    "mt_hgdp_tgp = mt_hgdp_tgp.annotate_entries(gvcf_info = mt_hgdp_tgp.gvcf_info.annotate(RAW_MQandDP =\n",
    "                                                        hl.array([mt_hgdp_tgp.gvcf_info.RAW_MQ,\n",
    "                                                                  mt_hgdp_tgp.gvcf_info.MQ_DP])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd28048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/29/2023 08:24:56 PM (__main__ 131): Computing AS_MQ as sqrt(AS_RAW_MQandDP[0]/AS_RAW_MQandDP[1]). Note that AS_MQ will be set to 0 if AS_RAW_MQandDP[1] == 0.\n",
      "08/29/2023 08:24:56 PM (__main__ 166): Computing AS_QD as AS_QUALapprox/AS_VarDP. Note that AS_QD will be set to 0 if AS_VarDP == 0.\n"
     ]
    }
   ],
   "source": [
    "# compute AS annotation\n",
    "as_ht_hgdp_tgp = default_compute_info(mt_hgdp_tgp)\n",
    "\n",
    "# only keep frequencies for the RankSum annotations\n",
    "as_ht_hgdp_tgp = as_table_cleanup(as_ht_hgdp_tgp)\n",
    "\n",
    "# as_ht_hgdp_tgp.filter(as_ht_hgdp_tgp.info.AS_ReadPosRankSum_freq[0][0] > 0).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b7e2b",
   "metadata": {},
   "source": [
    "### Check if all datasets contain the same AS annotations and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e16d8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nigeria_as = list(as_ht_nigeria.row) + list(as_ht_nigeria.info)\n",
    "neurogap_sa_as = list(as_ht_sa.row) + list(as_ht_sa.info)\n",
    "neurogap_extra_as = list(as_ht_neuro_extra.row) + list(as_ht_neuro_extra.info)\n",
    "ggv_as = list(as_ht_ggv.row) + list(as_ht_ggv.info)\n",
    "hgdp_tgp_as = list(as_ht_hgdp_tgp.row) + list(as_ht_hgdp_tgp.info)\n",
    "\n",
    "assert nigeria_as == neurogap_sa_as == neurogap_extra_as == ggv_as == hgdp_tgp_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24bee3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 20:48:20.220 Hail: INFO: wrote table with 94990636 rows in 4790 partitions to gs://gnomaf/AS_annotations/nigeria_54gene_merged_gvcfs.ht\n",
      "2023-08-29 20:57:49.972 Hail: INFO: wrote table with 126525458 rows in 4800 partitions to gs://gnomaf/AS_annotations/neurogap_south_africa_genomes_merged_gvcfs.ht\n",
      "2023-08-29 21:04:19.262 Hail: INFO: wrote table with 88618041 rows in 2586 partitions to gs://gnomaf/AS_annotations/neurogap_highcov_all_sites_93_genomes_merged_gvcfs.ht\n",
      "2023-08-29 21:20:37.209 Hail: INFO: wrote table with 61164017 rows in 4847 partitions to gs://gnomaf/AS_annotations/gambian_genomes_merged_gvcfs.ht\n",
      "2023-08-29 22:33:22.019 Hail: INFO: wrote table with 1538715096 rows in 5000 partitions to gs://gnomaf/AS_annotations/gnomad.genomes.v3.1.2.hgdp_1kg_subset.ht\n"
     ]
    }
   ],
   "source": [
    "as_ht_nigeria.write('gs://gnomaf/AS_annotations/nigeria_54gene_merged_gvcfs.ht')\n",
    "as_ht_sa.write('gs://gnomaf/AS_annotations/neurogap_south_africa_genomes_merged_gvcfs.ht')\n",
    "as_ht_neuro_extra.write('gs://gnomaf/AS_annotations/neurogap_highcov_all_sites_93_genomes_merged_gvcfs.ht')\n",
    "as_ht_ggv.write('gs://gnomaf/AS_annotations/gambian_genomes_merged_gvcfs.ht')\n",
    "as_ht_hgdp_tgp.write('gs://gnomaf/AS_annotations/gnomad.genomes.v3.1.2.hgdp_1kg_subset.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a745b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}